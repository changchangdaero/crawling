import json
import requests
from bs4 import BeautifulSoup
from collections import Counter

# ================================
# 1. ë„¤ì´ë²„ ë‰´ìŠ¤ API ì„¤ì •
# ================================

from dotenv import load_dotenv
import os

load_dotenv()

NAVER_CLIENT_ID = os.getenv("client_id")
NAVER_CLIENT_SECRET =  os.getenv("client_secret")


NAVER_URL = "https://openapi.naver.com/v1/search/news.json"

naver_headers = {
    "X-Naver-Client-Id": NAVER_CLIENT_ID,
    "X-Naver-Client-Secret": NAVER_CLIENT_SECRET,
}


def fetch_naver_news(query: str, display: int = 10, sort: str = "date"):
    """
    ë„¤ì´ë²„ ë‰´ìŠ¤ APIë¡œ íŠ¹ì • í‚¤ì›Œë“œì˜ ë‰´ìŠ¤ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°.
    return: items ë¦¬ìŠ¤íŠ¸ (ë„¤ì´ë²„ ì›ë³¸ JSONì˜ items í•„ë“œ)
    """
    params = {
        "query": query,
        "display": display,
        "sort": "date",  # "date": ìµœì‹ ìˆœ, "sim": ì •í™•ë„ìˆœ
    }

    res = requests.get(NAVER_URL, headers=naver_headers, params=params)
    print(f"[{query}] Naver API Status:", res.status_code)

    if res.status_code != 200:
        print("ë„¤ì´ë²„ API í˜¸ì¶œ ì‹¤íŒ¨:", res.text)
        raise SystemExit()

    data = res.json()
    items = data.get("items", [])
    print(f"[{query}] ì›ë³¸ ê¸°ì‚¬ ê°œìˆ˜:", len(items))
    return items


def clean_html_tags(text: str) -> str:
    """
    ë„¤ì´ë²„ ê²€ìƒ‰ ê²°ê³¼ title/descriptionì— ì„ì¸ <b> íƒœê·¸ ë“± ì œê±°
    """
    if not text:
        return ""
    return BeautifulSoup(text, "html.parser").get_text()


def build_article_list(items, query: str):
    """
    - originallinkê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ link ì‚¬ìš©
    - 'ì™„ì „ ë™ì¼í•œ URL ë¬¸ìì—´' ê¸°ì¤€ìœ¼ë¡œë§Œ ì¤‘ë³µ ì œê±°
    - ì–´ë–¤ URLì´ ëª‡ ë²ˆ ë‚˜ì™”ëŠ”ì§€ë„ ì¶œë ¥
    """
    articles = []
    urls = []  # ì¤‘ë³µ í†µê³„ìš©

    # 1) URL ëª¨ìœ¼ê¸° (ì¤‘ë³µ ì¹´ìš´íŠ¸ìš©)
    for item in items:
        raw_url = item.get("originallink") or item.get("link")
        if not raw_url:
            continue
        urls.append(raw_url)

    # 2) URL ì¤‘ë³µ í†µê³„
    counter = Counter(urls)
    print(f"\n=== [{query}] URL ì¤‘ë³µ í†µê³„ (ì›ë³¸ URL ê¸°ì¤€) ===")
    dup_exist = False
    for url, cnt in counter.items():
        if cnt > 1:
            dup_exist = True
            print(f"- {url} -> {cnt}ë²ˆ ë“±ì¥")
    if not dup_exist:
        print("  ì¤‘ë³µëœ URL ì—†ìŒ âœ…")

    # 3) ì‹¤ì œ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸(ì¤‘ë³µ ì œê±°)
    seen = set()
    for item in items:
        title_raw = item.get("title", "")
        title = clean_html_tags(title_raw)

        raw_url = item.get("originallink") or item.get("link")
        if not raw_url:
            print(f"[ìŠ¤í‚µ] URL ì—†ìŒ: {title}")
            continue

        if raw_url in seen:
            print(f"[ì¤‘ë³µ ìŠ¤í‚µ] {title} ({raw_url})")
            continue
        seen.add(raw_url)

        articles.append({
            "id": len(articles) + 1,
            "query": query,
            "title": title,
            "url": raw_url,
        })

    # 4) ìµœì¢… ê¸°ì‚¬ ëª©ë¡ ì¶œë ¥
    print(f"\n=== [{query}] ìµœì¢… ê¸°ì‚¬ ëª©ë¡ (ì¤‘ë³µ ì œê±° í›„) ===")
    for a in articles:
        print(f"  [{a['id']}] {a['title']}")
        print(f"       URL: {a['url']}")

    print(f"\nğŸ‘‰ (ì¤‘ë³µ ì œê±° í›„) [{query}] ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ ê°œìˆ˜: {len(articles)}")

    return articles


def main():
    # ì—¬ê¸°ì„œ ê²€ìƒ‰í•  í‚¤ì›Œë“œë“¤ì„ ì •í•´ì¤˜
    queries = ["ì‚¼ì„±ì „ì"]
    display = 20  # í‚¤ì›Œë“œë‹¹ ê°€ì ¸ì˜¬ ê¸°ì‚¬ ê°œìˆ˜

    all_articles = []

    for q in queries:
        items = fetch_naver_news(query=q, display=display, sort="date")
        article_list = build_article_list(items, query=q)
        all_articles.extend(article_list)

    # ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥ â†’ 2ë²ˆ íŒŒì¼ì—ì„œ ì´ê±¸ ì½ì–´ì„œ ë³¸ë¬¸ í¬ë¡¤ë§ì— ì‚¬ìš©
    output_file = "step1_naver_articles.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(all_articles, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… ì €ì¥ ì™„ë£Œ: {output_file}")
    print(f"   ì´ ê¸°ì‚¬ ìˆ˜: {len(all_articles)}")


if __name__ == "__main__":
    main()
