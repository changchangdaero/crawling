# step5_save_to_db.py
import json
import pymysql
from datetime import datetime

# ================================
# 0. DB ì ‘ì† ì„¤ì •
# ================================
DB_HOST = "127.0.0.1"
DB_PORT = 3306
DB_USER = "root"
DB_PASSWORD = "changmin"
DB_NAME = "test"   # HeidiSQLì—ì„œ ì“°ëŠ” DB ì´ë¦„

INPUT_FILE = "step4_articles_with_sentiment.json"


def get_connection():
    return pymysql.connect(
        host=DB_HOST,
        port=DB_PORT,
        user=DB_USER,
        password=DB_PASSWORD,
        database=DB_NAME,
        charset="utf8mb4",
        cursorclass=pymysql.cursors.DictCursor,
        autocommit=False,
    )


# ================================
# 1. í…Œì´ë¸” ì¤€ë¹„ (ERD ê¸°ë°˜ íŠœë‹ ë²„ì „)
# ================================
def ensure_tables(conn):
    create_companies_sql = """
    CREATE TABLE IF NOT EXISTS Companies (
        id BIGINT AUTO_INCREMENT PRIMARY KEY,
        name VARCHAR(50) NOT NULL UNIQUE,
        sector_id BIGINT NULL
    ) ENGINE=InnoDB
      DEFAULT CHARSET=utf8mb4
      COLLATE=utf8mb4_unicode_ci;
    """

    create_news_sql = """
    CREATE TABLE IF NOT EXISTS News (
        id BIGINT AUTO_INCREMENT PRIMARY KEY,
        title VARCHAR(500) NOT NULL,
        date DATETIME NOT NULL,
        full_text MEDIUMTEXT NOT NULL,
        url VARCHAR(1000) NOT NULL,
        company_id BIGINT NULL,
        UNIQUE KEY uq_news_url (url),
        INDEX idx_company_id (company_id),
        CONSTRAINT fk_news_company
          FOREIGN KEY (company_id) REFERENCES Companies(id)
          ON DELETE SET NULL
    ) ENGINE=InnoDB
      DEFAULT CHARSET=utf8mb4
      COLLATE=utf8mb4_unicode_ci;
    """

    create_sentiments_sql = """
    CREATE TABLE IF NOT EXISTS Sentiments (
        id BIGINT AUTO_INCREMENT PRIMARY KEY,
        label VARCHAR(50) NOT NULL,
        prob_pos FLOAT NOT NULL,
        prob_neg FLOAT NOT NULL,
        prob_neu FLOAT NOT NULL,
        score FLOAT NOT NULL,
        date DATETIME NOT NULL,
        news_id BIGINT NOT NULL,
        UNIQUE KEY uq_sentiments_news (news_id),
        INDEX idx_news_id (news_id),
        CONSTRAINT fk_sentiments_news
          FOREIGN KEY (news_id) REFERENCES News(id)
          ON DELETE CASCADE
    ) ENGINE=InnoDB
      DEFAULT CHARSET=utf8mb4
      COLLATE=utf8mb4_unicode_ci;
    """

    with conn.cursor() as cur:
        cur.execute(create_companies_sql)
        cur.execute(create_news_sql)
        cur.execute(create_sentiments_sql)
    conn.commit()


# ================================
# 2. JSON ë¡œë“œ + ë‚ ì§œ íŒŒì‹±
# ================================
def load_json(path: str):
    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)
    return data.get("articles", []), data.get("groups", [])


def parse_article_datetime(article) -> datetime:
    """
    Naver API pubDate / published_at ë“±ì„ DATETIMEìœ¼ë¡œ ë³€í™˜.
    ëª» ì½ìœ¼ë©´ ê·¸ëƒ¥ ì§€ê¸ˆ ì‹œê°„ìœ¼ë¡œ.
    """
    raw = (
        article.get("published_at")
        or article.get("pubDate")
        or article.get("date")
    )

    if not raw:
        return datetime.now()

    raw = str(raw).strip()

    # ìì£¼ ì“°ì´ëŠ” í¬ë§· ëª‡ ê°œ ì‹œë„
    for fmt in (
        "%Y-%m-%dT%H:%M:%S%z",
        "%Y-%m-%dT%H:%M:%S",
        "%a, %d %b %Y %H:%M:%S %z",  # Thu, 28 Nov 2024 09:03:00 +0900
    ):
        try:
            dt = datetime.strptime(raw, fmt)
            # DBì—ëŠ” timezone ì—†ëŠ” DATETIMEìœ¼ë¡œ ì €ì¥
            return dt.replace(tzinfo=None)
        except ValueError:
            continue

    # ë‹¤ ì‹¤íŒ¨í•˜ë©´ ê·¸ëƒ¥ ì§€ê¸ˆ ì‹œê°„
    return datetime.now()


# ================================
# 3. Companies / News / Sentiments ì €ì¥
# ================================
def save_articles_to_erd(conn, articles):
    """
    ERD êµ¬ì¡°ì— ë§ì¶° ì €ì¥:
      - Companies(name)  : query ê¸°ì¤€ìœ¼ë¡œ upsert
      - News             : ê¸°ì‚¬ ë³¸ë¬¸ / URL ì €ì¥ (URL UNIQUE)
      - Sentiments       : ê°ì • ì ìˆ˜ ì €ì¥ (news_id UNIQUE â€“ 1ê¸°ì‚¬ 1í–‰)
    """
    news_sql = """
    INSERT INTO News (
        title, date, full_text, url, company_id
    ) VALUES (
        %(title)s, %(date)s, %(full_text)s, %(url)s, %(company_id)s
    )
    ON DUPLICATE KEY UPDATE
        title      = VALUES(title),
        date       = VALUES(date),
        full_text  = VALUES(full_text),
        company_id = VALUES(company_id),
        id         = LAST_INSERT_ID(id);  -- ê¸°ì¡´ í–‰ì´ì–´ë„ lastrowidì— id ë“¤ì–´ì˜¤ê²Œ
    """

    sentiments_sql = """
    INSERT INTO Sentiments (
        label, prob_pos, prob_neg, prob_neu, score, date, news_id
    ) VALUES (
        %(label)s, %(prob_pos)s, %(prob_neg)s, %(prob_neu)s,
        %(score)s, %(date)s, %(news_id)s
    )
    ON DUPLICATE KEY UPDATE
        label    = VALUES(label),
        prob_pos = VALUES(prob_pos),
        prob_neg = VALUES(prob_neg),
        prob_neu = VALUES(prob_neu),
        score    = VALUES(score),
        date     = VALUES(date),
        id       = LAST_INSERT_ID(id);
    """

    with conn.cursor() as cur:
        for a in articles:
            # 1) íšŒì‚¬ ì´ë¦„(= query) â†’ Companies í…Œì´ë¸”ì— upsert
            company_name = (a.get("query") or "").strip()
            company_id = None

            if company_name:
                # ì´ë¯¸ ìˆëŠ”ì§€ í™•ì¸
                cur.execute(
                    "SELECT id FROM Companies WHERE name = %s",
                    (company_name,),
                )
                row = cur.fetchone()
                if row:
                    company_id = row["id"]
                else:
                    # ì—†ìœ¼ë©´ ìƒˆë¡œ INSERT
                    cur.execute(
                        "INSERT INTO Companies (name, sector_id) VALUES (%s, %s)",
                        (company_name, None),
                    )
                    company_id = cur.lastrowid

            # 2) ê¸°ì‚¬ ë‚ ì§œ / ì œëª© / ë³¸ë¬¸ / URL ì¤€ë¹„
            article_dt = parse_article_datetime(a)

            title = (a.get("title") or "").strip()
            if len(title) > 500:
                title = title[:500]

            url = (a.get("url") or "").strip()
            if len(url) > 1000:
                url = url[:1000]

            news_params = {
                "title": title,
                "date": article_dt,
                "full_text": a.get("content") or "",
                "url": url,
                "company_id": company_id,
            }

            # 3) News upsert (URL ê¸°ì¤€)
            cur.execute(news_sql, news_params)
            news_id = cur.lastrowid  # ìƒˆë¡œ insertë“  updateë“  ì—¬ê¸°ë¡œ ê¸°ì‚¬ PK í™•ë³´

            # 4) Sentiments upsert (news_id ê¸°ì¤€ 1í–‰)
            sentiment_params = {
                "label": a.get("sentiment_label") or "",
                "prob_pos": a.get("sentiment_prob_positive") or 0.0,
                "prob_neg": a.get("sentiment_prob_negative") or 0.0,
                "prob_neu": a.get("sentiment_prob_neutral") or 0.0,
                "score": a.get("sentiment_index") or 0.0,  # 0~100 ì§€í‘œ
                "date": article_dt,
                "news_id": news_id,
            }
            cur.execute(sentiments_sql, sentiment_params)

    conn.commit()
    print(f"âœ… ERD í…Œì´ë¸” ì €ì¥ ì™„ë£Œ (ì²˜ë¦¬ ê¸°ì‚¬ ìˆ˜: {len(articles)})")


# ================================
# 4. main
# ================================
def main():
    articles, groups = load_json(INPUT_FILE)
    print(f"ğŸ“¥ JSON ë¡œë“œ ì™„ë£Œ: articles={len(articles)}, groups={len(groups)}")

    conn = get_connection()
    try:
        ensure_tables(conn)
        save_articles_to_erd(conn, articles)
    finally:
        conn.close()

    print("ğŸ‰ DB ì €ì¥ ì „ì²´ ì™„ë£Œ! (Companies / News / Sentiments)")


if __name__ == "__main__":
    main()
