import json
from transformers import pipeline
from dotenv import load_dotenv
import os

# ================================
# 0. .envì—ì„œ HF í† í° ì½ê¸°
# ================================
load_dotenv()  # .env íŒŒì¼ ë¡œë“œ

HF_TOKEN = os.getenv("huggingface_api_token")  # .envì— ìˆëŠ” í‚¤ ì´ë¦„ì´ hf_tokenì´ë¼ê³  ê°€ì •
if not HF_TOKEN:
    raise RuntimeError("âŒ .env íŒŒì¼ì— hf_token ì´ ì—†ìŠµë‹ˆë‹¤. hfcl_token=... í˜•íƒœë¡œ ì¶”ê°€í•´ ì£¼ì„¸ìš”.")

# ================================
# 1. ê°ì •ë¶„ì„ ëª¨ë¸ ì„¤ì •
# ================================
MODEL_NAME = "DataWizardd/finbert-sentiment-ko"

print(f"ğŸ“¦ ê°ì •ë¶„ì„ ëª¨ë¸ ë¡œë”© ì¤‘: {MODEL_NAME}")
sentiment_pipe = pipeline(
    "text-classification",
    model=MODEL_NAME,
    token=HF_TOKEN,      # âœ… ì—¬ê¸°ì„œ HF í† í° ì‚¬ìš©
    top_k=None,          # return_all_scores=True ëŒ€ì‹  ê¶Œì¥ ë°©ì‹
)

# ================================
# 2. ì…ì¶œë ¥ íŒŒì¼
# ================================
INPUT_FILE = "step3_articles_with_summary_and_groups.json"  # 3ë‹¨ê³„ ê²°ê³¼
OUTPUT_FILE = "step4_articles_with_sentiment.json"          # 4ë‹¨ê³„ ìµœì¢… ê²°ê³¼


def load_step3(input_file: str):
    """
    step3 ê²°ê³¼ íŒŒì¼ êµ¬ì¡°:
    {
      "articles": [ {...}, {...}, ... ],
      "groups": [ {...}, {...}, ... ]
    }
    """
    with open(input_file, "r", encoding="utf-8") as f:
        data = json.load(f)

    articles = data.get("articles", [])
    groups = data.get("groups", [])

    print(f"ğŸ“¥ ê¸°ì‚¬ ê°œìˆ˜: {len(articles)}")
    print(f"ğŸ“¥ ê·¸ë£¹ ê°œìˆ˜: {len(groups)}")

    return articles, groups


def compute_k_index(p_pos: float, p_neu: float, p_neg: float):
    """
    0~100 ì ìˆ˜ ê³„ì‚°:

    base = ê¸ì • - ë¶€ì •
    confidence = 1 - ì¤‘ë¦½
    S = base * confidence
    score = (S + 1) / 2 * 100
    """

    base = p_pos - p_neg          # ê¸ì • - ë¶€ì •
    confidence = 1.0 - p_neu      # 1 - ì¤‘ë¦½
    S = base * confidence
    raw_score = (S + 1.0) / 2.0 * 100.0
    score = max(0.0, min(100.0, raw_score))  # 0~100 í´ë¨í”„

    if score >= 80:
        zone = "ê°•í•œ ë§¤ìˆ˜ ê°ì • (FOMO/ê³¼ì—´ ê°€ëŠ¥ êµ¬ê°„)"
    elif score >= 60:
        zone = "ë§¤ìˆ˜ ìš°ìœ„"
    elif score >= 40:
        zone = "ì¤‘ë¦½ êµ¬ê°„"
    elif score >= 20:
        zone = "ë§¤ìˆ˜ ë¹„ì¶”ì²œ"
    else:
        zone = "ê°•í•œ ë§¤ìˆ˜ ê¸ˆì§€"

    return score, zone


def analyze_sentiment(text: str):
    """
    í…ìŠ¤íŠ¸ í•˜ë‚˜ ë°›ì•„ì„œ ê°ì •ë¶„ì„ ì‹¤í–‰ + 0~100 ì§€í‘œ ê³„ì‚°.
    """
    if not text or not text.strip():
        return {
            "label": "UNKNOWN",
            "raw_score": 0.0,
            "prob_positive": 0.0,
            "prob_neutral": 1.0,
            "prob_negative": 0.0,
            "sentiment_index": 50.0,
            "sentiment_zone": "ë°ì´í„° ì—†ìŒ",
        }

    snippet = text.strip()
    if len(snippet) > 512:
        snippet = snippet[:512]

    try:
        # top_k=None â†’ ëª¨ë“  ë¼ë²¨ í™•ë¥  ë°˜í™˜
        # ê²°ê³¼ í˜•íƒœ: [[{"label": "...", "score": ...}, ...]]
        outputs = sentiment_pipe(snippet, truncation=True)[0]
    except Exception as e:
        print(f"   âš ï¸ ê°ì •ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return {
            "label": "ERROR",
            "raw_score": 0.0,
            "prob_positive": 0.0,
            "prob_neutral": 1.0,
            "prob_negative": 0.0,
            "sentiment_index": 50.0,
            "sentiment_zone": "ì˜¤ë¥˜",
        }

    p_pos = 0.0
    p_neu = 0.0
    p_neg = 0.0

    for item in outputs:
        label = item.get("label", "")
        score = float(item.get("score", 0.0))

        if label in ["ê¸ì •", "positive", "POSITIVE", "LABEL_2"]:
            p_pos = score
        elif label in ["ì¤‘ë¦½", "neutral", "NEUTRAL", "LABEL_1"]:
            p_neu = score
        elif label in ["ë¶€ì •", "negative", "NEGATIVE", "LABEL_0"]:
            p_neg = score

    if (p_pos + p_neu + p_neg) == 0.0:
        best = max(outputs, key=lambda x: x.get("score", 0.0))
        p_pos = float(best.get("score", 1.0))
        p_neu = 0.0
        p_neg = 0.0

    best_label_item = max(outputs, key=lambda x: x.get("score", 0.0))
    best_label = best_label_item.get("label", "UNKNOWN")
    best_score = float(best_label_item.get("score", 0.0))

    sentiment_index, sentiment_zone = compute_k_index(p_pos, p_neu, p_neg)

    return {
        "label": best_label,
        "raw_score": best_score,
        "prob_positive": p_pos,
        "prob_neutral": p_neu,
        "prob_negative": p_neg,
        "sentiment_index": sentiment_index,
        "sentiment_zone": sentiment_zone,
    }


def main():
    articles, groups = load_step3(INPUT_FILE)

    enriched_articles = []

    print("\n=== ê°ì •ë¶„ì„ ì‹œì‘ (KR-FinBERT ê¸°ë°˜ 0~100 ì§€í‘œ ê³„ì‚°) ===")

    for idx, a in enumerate(articles, start=1):
        aid = a.get("id")
        title = a.get("title")
        summary = (a.get("summary_ko") or "").strip()
        content = (a.get("content") or "").strip()

        print("\n" + "=" * 90)
        print(f"â–¶ [{idx}/{len(articles)}] ID={aid}")
        print(f"ì œëª©: {title}")

        if summary:
            target_text = summary
            print("   â†’ summary_ko ê¸°ë°˜ ê°ì •ë¶„ì„")
        elif content:
            target_text = content[:512]
            print("   â†’ summary_ko ì—†ìŒ, ë³¸ë¬¸ ì•ë¶€ë¶„ìœ¼ë¡œ ê°ì •ë¶„ì„")
        else:
            target_text = ""
            print("   â†’ ë¶„ì„í•  í…ìŠ¤íŠ¸ ì—†ìŒ, UNKNOWN ì²˜ë¦¬ ì˜ˆì •")

        sentiment_result = analyze_sentiment(target_text)

        print(
            f"   [ê°ì •ë¶„ì„ ê²°ê³¼] label={sentiment_result['label']}, "
            f"raw={sentiment_result['raw_score']:.4f}, "
            f"index={sentiment_result['sentiment_index']:.2f}, "
            f"zone={sentiment_result['sentiment_zone']}"
        )
        print(
            f"   [í™•ë¥ ] ê¸ì •={sentiment_result['prob_positive']:.3f}, "
            f"ì¤‘ë¦½={sentiment_result['prob_neutral']:.3f}, "
            f"ë¶€ì •={sentiment_result['prob_negative']:.3f}"
        )

        enriched = {
            **a,
            "sentiment_label": sentiment_result["label"],
            "sentiment_raw_score": sentiment_result["raw_score"],
            "sentiment_prob_positive": sentiment_result["prob_positive"],
            "sentiment_prob_neutral": sentiment_result["prob_neutral"],
            "sentiment_prob_negative": sentiment_result["prob_negative"],
            "sentiment_index": sentiment_result["sentiment_index"],
            "sentiment_zone": sentiment_result["sentiment_zone"],
        }
        enriched_articles.append(enriched)

    output_data = {
        "articles": enriched_articles,
        "groups": groups,
    }

    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)

    print("\nâœ… ê°ì •ë¶„ì„ ì™„ë£Œ (0~100 ì§€í‘œ í¬í•¨)")
    print(f"   ì´ ê¸°ì‚¬ ìˆ˜: {len(enriched_articles)}")
    print(f"   ì €ì¥ íŒŒì¼: {OUTPUT_FILE}")


if __name__ == "__main__":
    main()
