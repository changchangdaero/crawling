import json
import time
from newspaper import Article

# ================================
# 1. ì…ì¶œë ¥ íŒŒì¼ ì„¤ì •
# ================================

INPUT_FILE = "step1_naver_articles.json"          # 1ë‹¨ê³„ì—ì„œ ë§Œë“  íŒŒì¼ (id, query, title, url)
OUTPUT_FILE = "step2_articles_with_content.json"  # ë³¸ë¬¸ê¹Œì§€ í¬í•¨í•œ ê²°ê³¼ íŒŒì¼

# ================================
# 2. ë³¸ë¬¸ í¬ë¡¤ë§ í•¨ìˆ˜ (newspaper3k)
# ================================
def get_full_text(url: str) -> str:
    """
    ê¸°ì‚¬ URLì—ì„œ newspaper3kë¡œ ë³¸ë¬¸ ì „ì²´ë¥¼ ê°€ì ¸ì˜´.
    ì‹¤íŒ¨í•˜ë©´ ""(ë¹ˆ ë¬¸ìì—´) ë¦¬í„´.
    """
    try:
        article = Article(url, language="ko")
        article.download()
        article.parse()
        text = (article.text or "").strip()
        return text
    except Exception as e:
        print(f"[ê²½ê³ ] ë³¸ë¬¸ í¬ë¡¤ë§ ì‹¤íŒ¨: {url}")
        print("       ì‚¬ìœ :", e)
        return ""


# ================================
# 3. step1 ê²°ê³¼ ë¶ˆëŸ¬ì˜¤ê¸°
# ================================
def load_articles(input_file: str):
    """
    step1ì—ì„œ ë§Œë“  JSON íŒŒì¼ ë¡œë“œ.
    êµ¬ì¡° ì˜ˆì‹œ: [{id, query, title, url}, ...]
    """
    with open(input_file, "r", encoding="utf-8") as f:
        articles = json.load(f)
    print(f"ğŸ“¥ ë¡œë“œí•œ ê¸°ì‚¬ ê°œìˆ˜: {len(articles)}")
    return articles


# ================================
# 4. ê° ê¸°ì‚¬ì— ë³¸ë¬¸(content) ë¶™ì´ê¸°
# ================================
def crawl_contents(articles):
    """
    ê° ê¸°ì‚¬ì— ëŒ€í•´ urlë¡œ ë³¸ë¬¸ í¬ë¡¤ë§í•´ì„œ "content" í•„ë“œ ì¶”ê°€.
    """
    results = []
    total = len(articles)

    for idx, a in enumerate(articles, start=1):
        url = a.get("url")
        title = a.get("title")

        print("\n==============================")
        print(f"[{idx}/{total}] ì œëª©: {title}")
        print(f"URL: {url}")

        if not url:
            print("[ìŠ¤í‚µ] URL ì—†ìŒ")
            content = ""
        else:
            content = get_full_text(url)

        if content:
            print(f"[ë³¸ë¬¸ ê¸¸ì´] {len(content)}ì")
        else:
            print("[ë³¸ë¬¸ ì—†ìŒ ë˜ëŠ” í¬ë¡¤ë§ ì‹¤íŒ¨]")

        # step2 í˜•ì‹: id, query, title, url, content
        results.append(
            {
                "id": a.get("id"),
                "query": a.get("query"),
                "title": title,
                "url": url,
                "content": content,
            }
        )

        # ë„ˆë¬´ ë¹ ë¥´ê²Œ ì—°ë‹¬ì•„ ê¸ìœ¼ë©´ ë§‰í ìˆ˜ë„ ìˆìœ¼ë‹ˆ ì‚´ì§ ì‰¬ì–´ê°€ê¸° (ì›í•˜ë©´ ì£¼ì„ ì²˜ë¦¬í•´ë„ ë¨)
        time.sleep(0.5)

    return results


# ================================
# 5. ë©”ì¸ ì‹¤í–‰ë¶€
# ================================
def main():
    # 1) step1 ê²°ê³¼ ë¡œë“œ
    articles = load_articles(INPUT_FILE)
    if not articles:
        print("âš ï¸ ì²˜ë¦¬í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 2) ë³¸ë¬¸ í¬ë¡¤ë§
    print("\n=== ë„¤ì´ë²„ ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§ ì‹œì‘ (newspaper3k) ===")
    articles_with_content = crawl_contents(articles)

    # 3) ê²°ê³¼ ì €ì¥
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(articles_with_content, f, ensure_ascii=False, indent=2)

    print("\nâœ… ë³¸ë¬¸ í¬ë¡¤ë§ ì™„ë£Œ")
    print(f"   ì´ ê¸°ì‚¬ ìˆ˜: {len(articles_with_content)}")
    print(f"   ì €ì¥ íŒŒì¼: {OUTPUT_FILE}")


if __name__ == "__main__":
    main()
